var documenterSearchIndex = {"docs":
[{"location":"#Documentation","page":"Documentation","title":"Documentation","text":"","category":"section"},{"location":"#Main-Function","page":"Documentation","title":"Main Function","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"bo!","category":"page"},{"location":"#BOSS.bo!","page":"Documentation","title":"BOSS.bo!","text":"bo!(problem::BossProblem{Function}; kwargs...)\nx = bo!(problem::BossProblem{Missing}; kwargs...)\n\nRun the Bayesian optimization procedure to solve the given optimization problem or give a recommendation for the next evaluation point if problem.f == missing.\n\nArguments\n\nproblem::BossProblem: Defines the optimization problem.\n\nKeywords\n\nmodel_fitter::ModelFitter: Defines the algorithm used to estimate model parameters.\nacq_maximizer::AcquisitionMaximizer: Defines the algorithm used to maximize the acquisition function.\nacquisition::AcquisitionFunction: Defines the acquisition function maximized to select       promising candidates for further evaluation.\nterm_cond::TermCond: Defines the termination condition.\noptions::BossOptions: Defines miscellaneous options and hyperparameters.\n\nReferences\n\nBossProblem, ModelFitter, AcquisitionMaximizer, TermCond, BossOptions\n\nExamples\n\nSee 'https://github.com/Sheld5/BOSS.jl/tree/master/scripts' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"#Problem-Definition","page":"Documentation","title":"Problem Definition","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"BossProblem","category":"page"},{"location":"#BOSS.BossProblem","page":"Documentation","title":"BOSS.BossProblem","text":"BossProblem(; kwargs...)\n\nDefines the whole optimization problem for the BOSS algorithm.\n\nProblem Definition\n\nThere is some (noisy) blackbox function `y = f(x) = f_true(x) + ϵ` where `ϵ ~ Normal`.\n\nWe have some surrogate model `y = model(x) ≈ f_true(x)`\ndescribing our knowledge (or lack of it) about the blackbox function.\n\nWe wish to find `x ∈ domain` such that `fitness(f(x))` is maximized\nwhile satisfying the constraints `f(x) <= y_max`.\n\nKeywords\n\nfitness::Fitness: The fitness function. See Fitness.\nf::Union{Function, Missing}: The objective blackbox function.\ndomain::Domain: The domain of x. See Domain.\ny_max: The constraints on y. (See the definition above.)\nmodel::SurrogateModel: See SurrogateModel.\nnoise_std_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       of the noise standard deviations of each y dimension.\ndata::ExperimentData: The initial data of objective function evaluations.       See [ExperimentDataPrior].\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"#Fitness","page":"Documentation","title":"Fitness","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Fitness\nNoFitness\nLinFitness\nNonlinFitness","category":"page"},{"location":"#BOSS.Fitness","page":"Documentation","title":"BOSS.Fitness","text":"An abstract type for a fitness function measuring the quality of an output y of the objective function.\n\nFitness is used by the AcquisitionFunction to determine promising points for future evaluations.\n\nSee also: AcquisitionFunction, NoFitness, LinFitness, NonlinFitness\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.NoFitness","page":"Documentation","title":"BOSS.NoFitness","text":"NoFitness()\n\nPlaceholder for problems with no defined fitness. Problems with NoFitness can only be solved with AcquisitionFunction which does not use fitness.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.LinFitness","page":"Documentation","title":"BOSS.LinFitness","text":"LinFitness(coefs::AbstractVector{<:Real})\n\nUsed to define a linear fitness function  measuring the quality of an output y of the objective function.\n\nMay provide better performance than the more general NonlinFitness as some acquisition functions can be calculated analytically with linear fitness functions whereas this may not be possible with a nonlinear fitness function.\n\nSee also: NonlinFitness\n\nExample\n\nA fitness function f(y) = y[1] + a * y[2] + b * y[3] can be defined as:\n\njulia> LinFitness([1., a, b])\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.NonlinFitness","page":"Documentation","title":"BOSS.NonlinFitness","text":"NonlinFitness(fitness::Function)\n\nUsed to define a general nonlinear fitness function measuring the quality of an output y of the objective function.\n\nIf your fitness function is linear, use LinFitness instead for better performance.\n\nSee also: LinFitness\n\nExample\n\njulia> NonlinFitness(y -> cos(y[1]) + sin(y[2]))\n\n\n\n\n\n","category":"type"},{"location":"#Domain","page":"Documentation","title":"Domain","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Domain","category":"page"},{"location":"#BOSS.Domain","page":"Documentation","title":"BOSS.Domain","text":"Domain(; kwargs...)\n\nDescribes the optimization domain.\n\nKeywords\n\nbounds::AbstractBounds: The basic box-constraints on x. This field is mandatory.\ndiscrete::AbstractVector{<:Bool}: Can be used to designate some dimensions       of the domain as discrete.\ncons::Union{Nothing, Function}: Used to define arbitrary nonlinear constraints on x.       Feasible points x must satisfy all(cons(x) .> 0.). An appropriate acquisition       maximizer which can handle nonlinear constraints must be used if cons is provided.       (See AcquisitionMaximizer.)\n\n\n\n\n\n","category":"type"},{"location":"#Constraints-on-Output","page":"Documentation","title":"Constraints on Output","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"y_max","category":"page"},{"location":"#Surrogate-Model","page":"Documentation","title":"Surrogate Model","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"SurrogateModel\nParametric\nLinModel\nNonlinModel\nNonparametric\nGaussianProcess\nSemiparametric","category":"page"},{"location":"#BOSS.SurrogateModel","page":"Documentation","title":"BOSS.SurrogateModel","text":"An abstract type for a surrogate model approximating the objective function.\n\nExample usage: struct CustomModel <: SurrogateModel ... end\n\nAll models should implement the following methods: make_discrete(model::CustomModel, discrete::AbstractVector{<:Bool}) -> discrete_model::CustomModel model_posterior(model::CustomModel, data::ExperimentDataMLE; split::Bool) -> (x -> mean, std) <or> [(x -> mean_i, std_i) for i in 1:y_dim] model_posterior(model::CustomModel, data::ExperimentDataBI; split::Bool) -> [(x -> mean, std) for each sample] <or> [[(x -> mean_i, std_i) for i in 1:y_dim] for each sample] model_loglike(model::CustomModel, noise_std_priors::AbstractVector{<:UnivariateDistribution}, data::ExperimentData) -> (θ, length_scales, noise_std -> loglike) sample_params(model::CustomModel, noise_std_priors::AbstractVector{<:UnivariateDistribution}) -> (θ::AbstractVector{<:Real}, λ::AbstractMatrix{<:Real}, noise_std::AbstractVector{<:Real}) `parampriors(model::CustomModel) -> (θpriors::AbstractVector{<:UnivariateDistribution}, λ_priors::AbstractVector{<:MultivariateDistribution})\n\nSee also: LinModel, NonlinModel, GaussianProcess, Semiparametric\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.Parametric","page":"Documentation","title":"BOSS.Parametric","text":"An abstract type for parametric surrogate models.\n\nSee also: LinModel, NonlinModel\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.LinModel","page":"Documentation","title":"BOSS.LinModel","text":"LinModel(; kwargs...)\n\nA parametric surrogate model linear in its parameters.\n\nThis model definition will provide better performance than the more general 'NonlinModel' in the future. This feature is not implemented yet so it is equivalent to using NonlinModel for now.\n\nThe linear model is defined as     ϕs = lift(x)     y = [θs[i]' * ϕs[i] for i in 1:m] where     x = [x₁, ..., xₙ]     y = [y₁, ..., yₘ]     θs = [θ₁, ..., θₘ], θᵢ = [θᵢ₁, ..., θᵢₚ]     ϕs = [ϕ₁, ..., ϕₘ], ϕᵢ = [ϕᵢ₁, ..., ϕᵢₚ]      n, m, p ∈ R.\n\nKeywords\n\nlift::Function: Defines the lift function (::Vector{<:Real}) -> (::Vector{Vector{<:Real}})       according to the definition above.\nparam_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions for       the parameters [θ₁₁, ..., θ₁ₚ, ..., θₘ₁, ..., θₘₚ] according to the definition above.\ndiscrete::Union{Nothing, <:AbstractVector{<:Bool}}: Describes which dimensions are discrete.       Typically, the discrete kwarg can be ignored by the end-user as it will be updated       according to the Domain of the BossProblem during BOSS initialization.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.NonlinModel","page":"Documentation","title":"BOSS.NonlinModel","text":"NonlinModel(; kwargs...)\n\nA parametric surrogate model.\n\nIf your model is linear, you can use LinModel which will provide better performance in the future. (Not yet implemented.)\n\nDefine the model as y = predict(x, θ) where θ are the model parameters.\n\nKeywords\n\npredict::Function: The predict function according to the definition above.\nparam_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions for the model parameters.\ndiscrete::Union{Nothing, <:AbstractVector{<:Bool}}: Describes which dimensions are discrete.       Typically, the discrete kwarg can be ignored by the end-user as it will be updated       according to the Domain of the BossProblem during BOSS initialization.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.Nonparametric","page":"Documentation","title":"BOSS.Nonparametric","text":"Nonparametric(; kwargs...)\n\nAlias for GaussianProcess.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.GaussianProcess","page":"Documentation","title":"BOSS.GaussianProcess","text":"GaussianProcess(; kwargs...)\n\nA Gaussian Process surrogate model.\n\nKeywords\n\nmean::Union{Nothing, Function}: Used as the mean function for the GP.       Defaults to nothing equivalent to x -> [0.].\nkernel::Kernel: The kernel used in the GP. Defaults to the Matern32Kernel().\namp_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       for the amplitude hyperparameters of the GP. The amp_priors should be a vector       of y_dim univariate distributions.\nlength_scale_priors::AbstractVector{<:MultivariateDistribution}: The prior distributions       for the length scales of the GP. The length_scale_priors should be a vector       of y_dim x_dim-variate distributions where x_dim and y_dim are       the dimensions of the input and output of the model respectively.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.Semiparametric","page":"Documentation","title":"BOSS.Semiparametric","text":"Semiparametric(; kwargs...)\n\nA semiparametric surrogate model (a combination of a parametric model and Gaussian Process).\n\nThe parametric model is used as the mean of the Gaussian Process and all parameters and hyperparameters are estimated simultaneously.\n\nKeywords\n\nparametric::Parametric: The parametric model used as the GP mean function.\nnonparametric::Nonparametric{Nothing}: The outer GP model without mean.\n\n\n\n\n\n","category":"type"},{"location":"#Evaluation-Noise","page":"Documentation","title":"Evaluation Noise","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"noise_std_priors","category":"page"},{"location":"#Experiment-Data","page":"Documentation","title":"Experiment Data","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"ExperimentData\nExperimentDataPrior\nExperimentDataPost\nExperimentDataMLE\nExperimentDataBI","category":"page"},{"location":"#BOSS.ExperimentData","page":"Documentation","title":"BOSS.ExperimentData","text":"Stores all the data collected during the optimization as well as the parameters and hyperparameters of the model.\n\nSee also: ExperimentDataPrior, ExperimentDataPost\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ExperimentDataPrior","page":"Documentation","title":"BOSS.ExperimentDataPrior","text":"Stores the initial data.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\n\nSee also: ExperimentDataPost\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ExperimentDataPost","page":"Documentation","title":"BOSS.ExperimentDataPost","text":"Stores the fitted/samples model parameters in addition to the data matrices X,Y.\n\nSee also: ExperimentDataPrior, ExperimentDataMLE, ExperimentDataBI\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ExperimentDataMLE","page":"Documentation","title":"BOSS.ExperimentDataMLE","text":"Stores the data matrices X,Y as well as the optimized model parameters and hyperparameters.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\nθ::Union{Nothing, <:AbstractVector{<:Real}}: Contains the MLE parameters       of the parametric model (or nothing if the model is nonparametric).\nlength_scales::Union{Nothing, <:AbstractMatrix{<:Real}}: Contains the MLE length scales       of the GP as a x_dim×y_dim matrix (or nothing if the model is parametric).\namplitudes::Union{Nothing, <:AbstractVector{<:Real}}: Amplitudes of the GP.\nnoise_std::AbstractVector{<:Real}: The MLE noise standard deviations of each y dimension.\nconsistent::Bool: True iff the parameters (θ, length_scales, amplitudes, noise_std)       have been fitted using the current dataset (X, Y).       Is set to consistent = false after updating the dataset,       and to consistent = true after re-fitting the parameters.\n\nSee also: ExperimentDataBI\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ExperimentDataBI","page":"Documentation","title":"BOSS.ExperimentDataBI","text":"Stores the data matrices X,Y as well as the sampled model parameters and hyperparameters.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\nθ::Union{Nothing, <:AbstractVector{<:AbstractVector{<:Real}}}: Samples of parameters of the parametric model       stored column-wise in a matrix (or nothing if the model is nonparametric).\nlength_scales::Union{Nothing, <:AbstractVector{<:AbstractMatrix{<:Real}}}: Samples   of the length scales of the GP as a vector of x_dim×y_dim matrices   (or nothing if the model is parametric).\namplitudes::Union{Nothing, <:AbstractVector{<:AbstractVector{<:Real}}}: Samples of the amplitudes of the GP.\nnoise_std::AbstractVector{<:AbstractVector{<:Real}}: Samples of the noise standard deviations of each y dimension       stored column-wise in a matrix.\nconsistent::Bool: True iff the parameters (θ, length_scales, amplitudes, noise_std)       have been fitted using the current dataset (X, Y).       Is set to consistent = false after updating the dataset,       and to consistent = true after re-fitting the parameters.\n\nSee also: ExperimentDataMLE\n\n\n\n\n\n","category":"type"},{"location":"#Model-Fitter","page":"Documentation","title":"Model Fitter","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"ModelFitter\nModelFit\nOptimizationMLE\nTuringBI\nSamplingMLE\nRandomMLE","category":"page"},{"location":"#BOSS.ModelFitter","page":"Documentation","title":"BOSS.ModelFitter","text":"Specifies the library/algorithm used for model parameter estimation. Inherit this type to define a custom model-fitting algorithms.\n\nExample: struct CustomFitter <: ModelFitter{MLE} ... end or struct CustomFitter <: ModelFitter{BI} ... end\n\nStructures derived from this type have to implement the following method: estimate_parameters(model_fitter::CustomFitter, problem::BossProblem; info::Bool).\n\nThis method should return a named tuple (θ = ..., length_scales = ..., noise_std = ...) with either MLE model parameters (if CustomAlg <: ModelFitter{MLE}) or model parameter samples (if CustomAlg <: ModelFitter{BI}).\n\nSee also: OptimizationMLE, TuringBI\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ModelFit","page":"Documentation","title":"BOSS.ModelFit","text":"An abstract type used to differentiate between MLE (Maximum Likelihood Estimation) optimizers and BI (Bayesian Inference) samplers.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.OptimizationMLE","page":"Documentation","title":"BOSS.OptimizationMLE","text":"OptimizationMLE(; kwargs...)\n\nFinds the MLE of the model parameters and hyperparameters using the Optimization.jl package.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nsoftplus_hyperparams::Bool: If softplus_hyperparams=true then the softplus function       is applied to GP hyperparameters (length-scales & amplitudes) and noise deviations       to ensure positive values during optimization.\nsoftplus_params::Union{Bool, Vector{Bool}}: Defines to which parameters of the parametric       model should the softplus function be applied to ensure positive values.       Supplying a boolean instead of a binary vector turns the softplus on/off for all parameters.       Defaults to false meaning the softplus is applied to no parameters.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.TuringBI","page":"Documentation","title":"BOSS.TuringBI","text":"TuringBI(; kwargs...)\n\nSamples the model parameters and hyperparameters using the Turing.jl package.\n\nKeywords\n\nsampler::Any: The sampling algorithm used to draw the samples.\nn_adapts::Int: The amount of initial unused 'warmup' samples in each chain.\nsamples_in_chain::Int: The amount of samples used from each chain.\nchain_count::Int: The amount of independent chains sampled.\nleap_size: Every leap_size-th sample is used from each chain. (To avoid correlated samples.)\nparallel: If parallel=true then the chains are sampled in parallel.\n\nSampling Process\n\nIn each sampled chain;\n\nThe first n_adapts samples are discarded.\nFrom the following leap_size * samples_in_chain samples each leap_size-th is kept.\n\nThen the samples from all chains are concatenated and returned.\n\nTotal drawn samples:    'chaincount * (warmup + leapsize * samplesinchain)' Total returned samples: 'chaincount * samplesin_chain'\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.SamplingMLE","page":"Documentation","title":"BOSS.SamplingMLE","text":"SamplingMLE()\n\nOptimizes the model parameters by sampling them from their prior distributions and selecting the best sample in sense of MLE.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.RandomMLE","page":"Documentation","title":"BOSS.RandomMLE","text":"RandomMLE()\n\nReturns random model parameters sampled from their respective priors.\n\nCan be useful with RandomSelectAM to avoid unnecessary model parameter estimations.\n\n\n\n\n\n","category":"type"},{"location":"#Acquisition-Maximizer","page":"Documentation","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"AcquisitionMaximizer\nOptimizationAM\nGridAM\nRandomAM\nSequentialBatchAM","category":"page"},{"location":"#BOSS.AcquisitionMaximizer","page":"Documentation","title":"BOSS.AcquisitionMaximizer","text":"Specifies the library/algorithm used for acquisition function optimization. Inherit this type to define a custom acquisition maximizer.\n\nExample: struct CustomAlg <: AcquisitionMaximizer ... end\n\nStructures derived from this type have to implement the following method: maximize_acquisition(acq_maximizer::CustomAlg, acq::AcquisitionFunction, problem::BossProblem, options::BossOptions) This method should return the point of the input domain which maximizes the given acquisition function acq (as a vector) or a batch of points (as a column-wise matrix).\n\nSee also: OptimizationAM\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.OptimizationAM","page":"Documentation","title":"BOSS.OptimizationAM","text":"OptimizationAM(; kwargs...)\n\nMaximizes the acquisition function using the Optimization.jl library.\n\nCan handle constraints on x if according optimization algorithm is selected.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nautodiff:SciMLBase.AbstractADType:: The automatic differentiation module       passed to Optimization.OptimizationFunction.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.GridAM","page":"Documentation","title":"BOSS.GridAM","text":"GridAM(problem, steps; kwargs...)\n\nMaximizes the acquisition function by checking a fine grid of points from the domain.\n\nExtremely simple optimizer which can be used for simple problems or for debugging. Not suitable for problems with high dimensional domain.\n\nCan be used with constraints on x.\n\nArguments\n\nproblem::BossProblem: Provide your defined optimization problem.\nsteps::Vector{Float64}: Defines the size of the grid gaps in each x dimension.\n\nKeywords\n\nparallel::Bool: If parallel=true then the optimization is parallelized. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.RandomAM","page":"Documentation","title":"BOSS.RandomAM","text":"RandomAM()\n\nSelects a random interior point instead of maximizing the acquisition function. Can be used for method comparison.\n\nCan handle constraints on x, but does so by generating random points in the box domain until a point satisfying the constraints is found. Therefore it can take a long time or even get stuck if the constraints are very tight.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.SequentialBatchAM","page":"Documentation","title":"BOSS.SequentialBatchAM","text":"SequentialBatchAM(::AcquisitionMaximizer, ::Int)\nSequentialBatchAM(; am, batch_size)\n\nProvides multiple candidates for batched objective function evaluation.\n\nSelects the candidates sequentially by iterating the following steps:\n\n) Use the 'inner' acquisition maximizer to select a candidatex`.\nExtend the dataset with a 'speculative' new data point\ncreated by taking the candidate x and the posterior predictive mean of the surrogate ŷ.\nIf batch_size candidates have been selected, return them.\nOtherwise, goto step 1).\n\nKeywords\n\nam::AcquisitionMaximizer: The inner acquisition maximizer.\nbatch_size::Int: The number of candidates to be selected.\n\n\n\n\n\n","category":"type"},{"location":"#Acquisition-Function","page":"Documentation","title":"Acquisition Function","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"AcquisitionFunction\nExpectedImprovement","category":"page"},{"location":"#BOSS.AcquisitionFunction","page":"Documentation","title":"BOSS.AcquisitionFunction","text":"Specifies the acquisition function describing the \"quality\" of a potential next evaluation point. Inherit this type to define a custom acquisition function.\n\nExample: struct CustomAcq <: AcquisitionFunction ... end\n\nStructures derived from this type have to implement the following method: (acquisition::CustomAcq)(problem::BossProblem, options::BossOptions)\n\nThis method should return a function acq(x::AbstractVector{<:Real}) = val::Real, which is maximized to select the next evaluation function of blackbox function in each iteration.\n\nSee also: ExpectedImprovement\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.ExpectedImprovement","page":"Documentation","title":"BOSS.ExpectedImprovement","text":"ExpectedImprovement(; kwargs...)\n\nThe expected improvement (EI) acquisition function.\n\nFitness function must be defined as a part of the problem definition in order to use EI. (See Fitness.)\n\nMeasures the quality of a potential evaluation point x as the expected improvement in best-so-far achieved fitness by evaluating the objective function at y = f(x).\n\nIn case of constrained problems, the expected improvement is additionally weighted by the probability of feasibility of y. I.e. the probability that all(cons(y) .> 0.).\n\nIf the problem is constrained on y and no feasible point has been observed yet, then the probability of feasibility alone is returned as the acquisition function.\n\nRather than using the actual evaluations (xᵢ,yᵢ) from the dataset, the best-so-far achieved fitness is calculated as the maximum fitness among the means ̂yᵢ of the posterior predictive distribution of the model evaluated at xᵢ. This is a simple way to handle evaluation noise which may not be suitable for problems with substantial noise. In case of Bayesian Inference, an averaged posterior of the model posterior samples is used for the prediction of ŷᵢ.\n\nKeywords\n\nϵ_samples::Int: Controls how many samples are used to approximate EI.       The ϵ_samples keyword is ignored unless MLE model fitter and NonlinFitness are used!       In case of BI model fitter, the number of samples is instead set equal to the number of posterior samples.       In case of LinearFitness, the expected improvement can be calculated analytically.\ncons_safe::Bool: If set to true, the acquisition function acq(x) is made 'constraint-safe'       by checking the bounds and constraints during each evaluation.       Set cons_safe to true if the evaluation of the model at exterior points       may cause errors or nonsensical values.       You may set cons_safe to false if the evaluation of the model at exterior points       can provide useful information to the acquisition maximizer and does not cause errors.       Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"#Termination-Conditions","page":"Documentation","title":"Termination Conditions","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"TermCond\nIterLimit","category":"page"},{"location":"#BOSS.TermCond","page":"Documentation","title":"BOSS.TermCond","text":"Specifies the termination condition of the whole BOSS algorithm. Inherit this type to define a custom termination condition.\n\nExample: struct CustomCond <: TermCond ... end\n\nStructures derived from this type have to implement the following method: (cond::CustomCond)(problem::BossProblem)\n\nThis method should return true to keep the optimization running and return false once the optimization is to be terminated.\n\nSee also: IterLimit\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.IterLimit","page":"Documentation","title":"BOSS.IterLimit","text":"IterLimit(iter_max::Int)\n\nTerminates the BOSS algorithm after predefined number of iterations.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"#Miscellaneous","page":"Documentation","title":"Miscellaneous","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"result\nBossOptions\nBossCallback\nNoCallback\nPlotCallback","category":"page"},{"location":"#BOSS.result","page":"Documentation","title":"BOSS.result","text":"result(problem) -> (x, y)\n\nReturn the best found point (x, y).\n\nReturns the point (x, y) from the dataset of the given problem such that y satisfies the constraints and fitness(y) is maximized. Returns nothing if the dataset is empty or if no feasible point is present.\n\nDoes not check whether x belongs to the domain as no exterior points should be present in the dataset.\n\n\n\n\n\n","category":"function"},{"location":"#BOSS.BossOptions","page":"Documentation","title":"BOSS.BossOptions","text":"BossOptions(; kwargs...)\n\nStores miscellaneous settings of the BOSS algorithm.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the BOSS algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::BossCallback: If provided, callback(::BossProblem; kwargs...)       will be called before the BO procedure starts and after every iteration.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.BossCallback","page":"Documentation","title":"BOSS.BossCallback","text":"If an object cb of type BossCallback is passed to BossOptions, the method shown below will be called before the BO procedure starts and after every iteration.\n\ncb(problem::BossProblem;\n    model_fitter::ModelFitter,\n    acq_maximizer::AcquisitionMaximizer,\n    acquisition::AcquisitionFunction,\n    term_cond::TermCond,\n    options::BossOptions,\n    first::Bool,\n)\n\nThe kwargs first is true on the first callback before the BO procedure starts, and is false on all subsequent callbacks after each iteration.\n\nSee PlotCallback for an example usage of this feature for plotting.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.NoCallback","page":"Documentation","title":"BOSS.NoCallback","text":"NoCallback()\n\nDoes nothing.\n\n\n\n\n\n","category":"type"},{"location":"#BOSS.PlotCallback","page":"Documentation","title":"BOSS.PlotCallback","text":"PlotOptions(Plots; kwargs...)\n\nIf PlotOptions is passed to BossOptions as callback, the state of the optimization problem is plotted in each iteration. Only works with one-dimensional x domains but supports multi-dimensional y.\n\nArguments\n\nPlots::Module: Evaluate using Plots and pass the Plots module to PlotsOptions.\n\nKeywords\n\nf_true::Union{Nothing, Function}: The true objective function to be plotted.\npoints::Int: The number of points in each plotted function.\nxaxis::Symbol: Used to change the x axis scale (:identity, :log).\nyaxis::Symbol: Used to change the y axis scale (:identity, :log).\ntitle::String: The plot title.\n\n\n\n\n\n","category":"type"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"An illustrative problem is solved using BOSS.jl here. The source code is available at github.","category":"page"},{"location":"example/#The-Problem","page":"Example","title":"The Problem","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We have an expensive-to-evaluate blackbox function blackbox(x) -> (y, z).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function blackbox(x; noise_std=0.1)\n    y = exp(x[1]/10) * cos(2*x[1])\n    z = (1/2)^6 * (x[1]^2 - (15.)^2)\n    \n    y += rand(Normal(0., noise_std))\n    z += rand(Normal(0., noise_std))\n\n    return [y,z]\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We wish to maximize y such that x ∈ <0, 20> (constraint on input) and z < 0 (constraint on output).","category":"page"},{"location":"example/#Problem-Definition","page":"Example","title":"Problem Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"First, we define the problem as an instance of BossProblem.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"problem() = BossProblem(;\n    fitness = LinFitness([1, 0]),           # maximize y\n    f = blackbox,\n    domain = Domain(;\n        bounds = ([0.], [20.]),             # x ∈ <0, 20>\n    ),\n    y_max = [Inf, 0.],                      # z < 0\n    model = nonparametric(), # or `parametric()` or `semiparametric()`\n    noise_std_priors = noise_std_priors(),\n    data = init_data(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use Fitness to define the objective. Here, the LinFitness([1, 0]) specifies that we wish to maximize 1*y + 0*z. (See also NonlinFitness.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword f to provide the blackbox objective function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the Domain structure to define the constraints on inputs.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword y_max to define the constraints on outputs.","category":"page"},{"location":"example/#Surrogate-Model-Definition","page":"Example","title":"Surrogate Model Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Now we define the surrogate model used to approximate the objective function based on the available data from previous evaluations.","category":"page"},{"location":"example/#Gaussian-Process","page":"Example","title":"Gaussian Process","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Usually, we will use a Gaussian process.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"nonparametric() = GaussianProcess(;\n    kernel = BOSS.Matern32Kernel(),\n    amp_priors = amplitude_priors(),\n    length_scale_priors = length_scale_priors(),\n)","category":"page"},{"location":"example/#Parametric-Model","page":"Example","title":"Parametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"If we have some knowledge about the blackbox function, we can define a parametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"parametric() = NonlinModel(;\n    predict = (x, θ) -> [\n        θ[1] * x[1] * cos(θ[2] * x[1]) + θ[3],\n        0.,\n    ],\n    param_priors = fill(Normal(0., 1.), 3),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The function predict(x, θ) -> y defines our parametric model where θ are the model parameters which will be fitted based on the data.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The keyword param_priors is used to define priors on the model parameters θ. The priors can be used to include our expert knowledge, to regularize the model, or a uniform prior can be used to not bias the model fit.","category":"page"},{"location":"example/#Semiparametric-Model","page":"Example","title":"Semiparametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can use the parametric model together with a Gaussian process to define the semiparametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"semiparametric() = Semiparametric(\n    parametric(),\n    nonparametric(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"This allows us to leverage our expert knowledge incorporated in the parametric model while benefiting from the flexibility of the Gaussian process.","category":"page"},{"location":"example/#Hyperparameter-Priors","page":"Example","title":"Hyperparameter Priors","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We need to define all hyperparameters. Instead of defining scalar values, we will define priors over them and let BOSS fit their values based on the data. This alleviates the importance of our choice and allows for Bayesian inference if we wish to use it.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(If one wants to define some hyperparameters as scalars instead, a Dirac prior can be used and the hyperparameters will be skipped from model fitting.)","category":"page"},{"location":"example/#Evaluation-Noise","page":"Example","title":"Evaluation Noise","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"BOSS assumes Gaussian evaluation noise on the objective blackbox function. Noise std priors define our belief about the standard deviation of the noise of each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"noise_std_priors() = fill(truncated(Normal(0., 0.1); lower=0.), 2)\n# noise_std_priors() = fill(Dirac(0.1), 2)","category":"page"},{"location":"example/#Amplitude","page":"Example","title":"Amplitude","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The amplitude of the Gaussian process expresses the expected deviation of the output values. We again define an amplitude prior for each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"amplitude_priors() = fill(truncated(Normal(0., 5.); lower=0.), 2)\n# amplitude_priors() = fill(Dirac(5.), 2)","category":"page"},{"location":"example/#Length-Scales","page":"Example","title":"Length Scales","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Informally, the length scales of the Gaussian process define how far within the input domain does the model extrapolate the information obtained from the dataset. For each output dimension, we define a multivariate prior over all input dimensions. (In our case two 1-variate priors.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"length_scale_priors() = fill(Product([truncated(Normal(0., 20/3); lower=0.)]), 2)\n# length_scale_priors() = fill(Product(fill(Dirac(1.), 1)), 2)","category":"page"},{"location":"example/#Model-Fitter","page":"Example","title":"Model Fitter","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to fit the model hyperparameters using the ModelFitter type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can fit the hyperparameters in a MLE fashion using the OptimizationMLE model fitter together with any algorithm from Optimization.jl and its extensions.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using OptimizationPRIMA\n\nmle_fitter() = OptimizationMLE(;\n    algorithm = NEWUOA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Or we can use Bayesian inference and sample the parameters from their posterior (given by the priors and the data likelihood) using the TuringBI model fitter.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bi_fitter() = TuringBI(;\n    sampler = BOSS.PG(20),\n    warmup = 100,\n    samples_in_chain = 10,\n    chain_count = 8,\n    leap_size = 5,\n    parallel = true,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also SamplingMLE and RandomMLE for more trivial model fitters suitable for simple experimentation with the package.","category":"page"},{"location":"example/#Acquisition-Maximizer","page":"Example","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to maximize the acquisition function (in order to select the next evaluation point) by using the AcquisitionMaximizer type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can use the OptimizationAM maximizer together with any algorithm from Optimization.jl.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"acq_maximizer() = OptimizationAM(;\n    algorithm = BOBYQA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Make sure to use an algorithm suitable for the given domain. (For example, in our case the domain is bounded by box constraints only, so we the BOBYQA optimization algorithm designed for box constraints problems.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also GridAM, RandomAM for more trivial acquisition maximizers suitable for simple experimentation with the package.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The SequentialBatchAM can be used to wrap any of the other acquisition maximizers to enable objective function evaluation in batches.","category":"page"},{"location":"example/#Acquisition-Function","page":"Example","title":"Acquisition Function","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The acquisition function defines how the next evaluation point is selected in each iteration. The acquisition function is maximized by the acquisition maximizer algorithm (discussed in the previous section).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Currently, the only implemented acquisition function is the ExpectedImprovement acquisition most commonly used in Bayesian optimization.","category":"page"},{"location":"example/#Miscellaneous","page":"Example","title":"Miscellaneous","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we can define the termination condition using the TermCond type. Currently, the only available termination condition is the trivial IterLimit condition. (However, one can simply define his own termination condition by extending the TermCond type.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The BossOptions structure can be used to change other miscellaneous settings.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The PlotCallback can be provided in BossOptions to enable plotting of the BO procedure. This can be useful for initial experimentation with the package. Note that the plotting only works for 1-dimensional input domains.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Plots\n\noptions() = BossOptions(;\n    info = true,\n    callback = PlotCallback(Plots;\n        f_true = x->blackbox(x; noise_std=0.),\n    ),\n)","category":"page"},{"location":"example/#Run-BOSS","page":"Example","title":"Run BOSS","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Once we define the problem and all hyperparameters, we can run the BO procedure by calling the bo! function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bo!(problem();\n    model_fitter = mle_fitter(), # or `bi_fitter()`\n    acq_maximizer = acq_maximizer(),\n    acquisition = ExpectedImprovement(),\n    term_cond = IterLimit(10),\n    options = options(),\n)","category":"page"}]
}
