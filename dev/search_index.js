var documenterSearchIndex = {"docs":
[{"location":"types/#Data-Types-and-Structures","page":"Data Types & Structures","title":"Data Types & Structures","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The following diagram showcases the type hierarchy of all BOSS inputs and hyperparameters.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"    \n  (Image: BOSS Pipeline)  \n    ","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"This reminder of this page contains documentation for all exported types and structures.","category":"page"},{"location":"types/#Problem-Definition","page":"Data Types & Structures","title":"Problem Definition","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossProblem structure contains the whole problem definition, the model definition, and the data together with the current parameter and hyperparameter values.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"BossProblem","category":"page"},{"location":"types/#BOSS.BossProblem","page":"Data Types & Structures","title":"BOSS.BossProblem","text":"BossProblem(; kwargs...)\n\nDefines the whole optimization problem for the BOSS algorithm.\n\nProblem Definition\n\nThere is some (noisy) blackbox function `y = f(x) = f_true(x) + ϵ` where `ϵ ~ Normal`.\n\nWe have some surrogate model `y = model(x) ≈ f_true(x)`\ndescribing our knowledge (or lack of it) about the blackbox function.\n\nWe wish to find `x ∈ domain` such that `fitness(f(x))` is maximized\nwhile satisfying the constraints `f(x) <= y_max`.\n\nKeywords\n\nfitness::Fitness: The fitness function. See Fitness.\nf::Union{Function, Missing}: The objective blackbox function.\ndomain::Domain: The domain of x. See Domain.\ny_max: The constraints on y. (See the definition above.)\nmodel::SurrogateModel: See SurrogateModel.\nnoise_std_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       of the noise standard deviations of each y dimension.\ndata::ExperimentData: The initial data of objective function evaluations.       See [ExperimentDataPrior].\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"types/#Fitness","page":"Data Types & Structures","title":"Fitness","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Fitness type is used to define the fitness function textfit(y) rightarrow mathbbR.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The NoFitness can be used in problems without defined fitness (such as active learning problems). It is the default option used if no fitness is provided to BossProblem. The NoFitness can only be used with AcquisitionFunction that does not require fitness.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The LinFitness can be used to define a simple linear fitness function","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"textfit(y) = alpha^T y ","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Using LinFitness instead of NonlinFitness may allow for simpler/faster computation of some acquisition functions.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The NonlinFitness can be used to define an arbitrary fitness function","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"textfit(y) rightarrow mathbbR ","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Fitness\nNoFitness\nLinFitness\nNonlinFitness","category":"page"},{"location":"types/#BOSS.Fitness","page":"Data Types & Structures","title":"BOSS.Fitness","text":"An abstract type for a fitness function measuring the quality of an output y of the objective function.\n\nFitness is used by the AcquisitionFunction to determine promising points for future evaluations.\n\nSee also: AcquisitionFunction, NoFitness, LinFitness, NonlinFitness\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NoFitness","page":"Data Types & Structures","title":"BOSS.NoFitness","text":"NoFitness()\n\nPlaceholder for problems with no defined fitness. Problems with NoFitness can only be solved with AcquisitionFunction which does not use fitness.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.LinFitness","page":"Data Types & Structures","title":"BOSS.LinFitness","text":"LinFitness(coefs::AbstractVector{<:Real})\n\nUsed to define a linear fitness function  measuring the quality of an output y of the objective function.\n\nMay provide better performance than the more general NonlinFitness as some acquisition functions can be calculated analytically with linear fitness functions whereas this may not be possible with a nonlinear fitness function.\n\nSee also: NonlinFitness\n\nExample\n\nA fitness function f(y) = y[1] + a * y[2] + b * y[3] can be defined as:\n\njulia> LinFitness([1., a, b])\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NonlinFitness","page":"Data Types & Structures","title":"BOSS.NonlinFitness","text":"NonlinFitness(fitness::Function)\n\nUsed to define a general nonlinear fitness function measuring the quality of an output y of the objective function.\n\nIf your fitness function is linear, use LinFitness instead for better performance.\n\nSee also: LinFitness\n\nExample\n\njulia> NonlinFitness(y -> cos(y[1]) + sin(y[2]))\n\n\n\n\n\n","category":"type"},{"location":"types/#Input-Domain","page":"Data Types & Structures","title":"Input Domain","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Domain structure is used to define the input domain x in textDomain. The domain is formalized as","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"beginaligned\n lb  x  ub \n d_i implies (x_i in mathbbZ) \n textcons(x)  0 \nendaligned","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Domain\nAbstractBounds","category":"page"},{"location":"types/#BOSS.Domain","page":"Data Types & Structures","title":"BOSS.Domain","text":"Domain(; kwargs...)\n\nDescribes the optimization domain.\n\nKeywords\n\nbounds::AbstractBounds: The basic box-constraints on x. This field is mandatory.\ndiscrete::AbstractVector{<:Bool}: Can be used to designate some dimensions       of the domain as discrete.\ncons::Union{Nothing, Function}: Used to define arbitrary nonlinear constraints on x.       Feasible points x must satisfy all(cons(x) .> 0.). An appropriate acquisition       maximizer which can handle nonlinear constraints must be used if cons is provided.       (See AcquisitionMaximizer.)\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.AbstractBounds","page":"Data Types & Structures","title":"BOSS.AbstractBounds","text":"bounds = ([0, 0], [1, 1])\n\nconst AbstractBounds = Tuple{<:AbstractVector{<:Real}, <:AbstractVector{<:Real}}\n\nDefines box constraints.\n\n\n\n\n\n","category":"type"},{"location":"types/#Output-Constraints","page":"Data Types & Structures","title":"Output Constraints","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Constraints on output vector y can be defined using the y_max field. Providing y_max to BossProblem defines the linear constraints y < y_max.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Arbitrary nonlinear constraints can be defined by augmenting the objective function. For example to define the constraint y[1] * y[2] < c, one can define an augmented objective function","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"function f_c(x)\n    y = f(x)  # the original objective function\n    y_c = [y..., y[1] * y[2]]\n    return y_c\nend","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"and use","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"y_max = [fill(Inf, y_dim)..., c]","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"where y_dim is the output dimension of the original objective function. Note that defining nonlinear constraints this way increases the output dimension of the objective function and thus the model definition has to be modified accordingly.","category":"page"},{"location":"types/#Surrogate-Model","page":"Data Types & Structures","title":"Surrogate Model","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The surrogate model is defined using the SurrogateModel type.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SurrogateModel","category":"page"},{"location":"types/#BOSS.SurrogateModel","page":"Data Types & Structures","title":"BOSS.SurrogateModel","text":"An abstract type for a surrogate model approximating the objective function.\n\nExample usage: struct CustomModel <: SurrogateModel ... end\n\nAll models should implement the following methods:\n\nmake_discrete(model::CustomModel, discrete::AbstractVector{<:Bool}) -> discrete_model::CustomModel\nmodel_posterior(model::CustomModel, data::ExperimentDataMAP) -> (x -> mean, std)\nmodel_posterior_slice(model::CustomModel, data::ExperimentDataMAP, slice::Int) -> (x -> mean, std)\nmodel_loglike(model::CustomModel, noise_std_priors::AbstractVector{<:UnivariateDistribution}, data::ExperimentData) -> (θ, length_scales, noise_std -> loglike)\nsample_params(model::CustomModel, noise_std_priors::AbstractVector{<:UnivariateDistribution}) -> (θ::AbstractVector{<:Real}, λ::AbstractMatrix{<:Real}, noise_std::AbstractVector{<:Real})\nparam_priors(model::CustomModel) -> (θ_priors::AbstractVector{<:UnivariateDistribution}, λ_priors::AbstractVector{<:MultivariateDistribution})\n\nSee also: LinModel, NonlinModel, GaussianProcess, Semiparametric\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The LinModel and NonlinModel structures are used to define parametric models. (Some compuatations are simpler/faster with linear model, so the LinModel might provide better performance in the future. This functionality is not implemented yet, so using the NonlinModel is equiavalent for now.)","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Parametric\nLinModel\nNonlinModel","category":"page"},{"location":"types/#BOSS.Parametric","page":"Data Types & Structures","title":"BOSS.Parametric","text":"An abstract type for parametric surrogate models.\n\nSee also: LinModel, NonlinModel\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.LinModel","page":"Data Types & Structures","title":"BOSS.LinModel","text":"LinModel(; kwargs...)\n\nA parametric surrogate model linear in its parameters.\n\nThis model definition will provide better performance than the more general 'NonlinModel' in the future. This feature is not implemented yet so it is equivalent to using NonlinModel for now.\n\nThe linear model is defined as\n\n    ϕs = lift(x)\n    y = [θs[i]' * ϕs[i] for i in 1:m]\n\nwhere\n\n    x = [x₁, ..., xₙ]\n    y = [y₁, ..., yₘ]\n    θs = [θ₁, ..., θₘ], θᵢ = [θᵢ₁, ..., θᵢₚ]\n    ϕs = [ϕ₁, ..., ϕₘ], ϕᵢ = [ϕᵢ₁, ..., ϕᵢₚ]\n\nand n m p  R.\n\nKeywords\n\nlift::Function: Defines the lift function (::Vector{<:Real}) -> (::Vector{Vector{<:Real}})       according to the definition above.\nparam_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions for       the parameters [θ₁₁, ..., θ₁ₚ, ..., θₘ₁, ..., θₘₚ] according to the definition above.\ndiscrete::Union{Nothing, <:AbstractVector{<:Bool}}: Describes which dimensions are discrete.       Typically, the discrete kwarg can be ignored by the end-user as it will be updated       according to the Domain of the BossProblem during BOSS initialization.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NonlinModel","page":"Data Types & Structures","title":"BOSS.NonlinModel","text":"NonlinModel(; kwargs...)\n\nA parametric surrogate model.\n\nIf your model is linear, you can use LinModel which will provide better performance in the future. (Not yet implemented.)\n\nDefine the model as y = predict(x, θ) where θ are the model parameters.\n\nKeywords\n\npredict::Function: The predict function according to the definition above.\nparam_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions for the model parameters.\ndiscrete::Union{Nothing, <:AbstractVector{<:Bool}}: Describes which dimensions are discrete.       Typically, the discrete kwarg can be ignored by the end-user as it will be updated       according to the Domain of the BossProblem during BOSS initialization.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The GaussianProcess structure is used to define a Gaussian process model. See [1] for more information about Gaussian processes.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Nonparametric\nGaussianProcess","category":"page"},{"location":"types/#BOSS.Nonparametric","page":"Data Types & Structures","title":"BOSS.Nonparametric","text":"Nonparametric(; kwargs...)\n\nAlias for GaussianProcess.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.GaussianProcess","page":"Data Types & Structures","title":"BOSS.GaussianProcess","text":"GaussianProcess(; kwargs...)\n\nA Gaussian Process surrogate model.\n\nKeywords\n\nmean::Union{Nothing, Function}: Used as the mean function for the GP.       Defaults to nothing equivalent to x -> [0.].\nkernel::Kernel: The kernel used in the GP. Defaults to the Matern32Kernel().\namp_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       for the amplitude hyperparameters of the GP. The amp_priors should be a vector       of y_dim univariate distributions.\nlength_scale_priors::AbstractVector{<:MultivariateDistribution}: The prior distributions       for the length scales of the GP. The length_scale_priors should be a vector       of y_dim x_dim-variate distributions where x_dim and y_dim are       the dimensions of the input and output of the model respectively.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Semiparametric structure is used to define a semiparametric model combining the parametric and nonparametric (Gaussian process) models.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Semiparametric","category":"page"},{"location":"types/#BOSS.Semiparametric","page":"Data Types & Structures","title":"BOSS.Semiparametric","text":"Semiparametric(; kwargs...)\n\nA semiparametric surrogate model (a combination of a parametric model and Gaussian Process).\n\nThe parametric model is used as the mean of the Gaussian Process and all parameters and hyperparameters are estimated simultaneously.\n\nKeywords\n\nparametric::Parametric: The parametric model used as the GP mean function.\nnonparametric::Nonparametric{Nothing}: The outer GP model without mean.\n\n\n\n\n\n","category":"type"},{"location":"types/#Evaluation-Noise","page":"Data Types & Structures","title":"Evaluation Noise","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The priors on evaluation noise deviation sigma_f are defined using the noise_std_priors field of the BossProblem.","category":"page"},{"location":"types/#Experiment-Data","page":"Data Types & Structures","title":"Experiment Data","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The data from all past objective function evaluations as well as estimated parameter and/or hyperparameter values of the surrogate model are stored in the ExperimentData types.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"ExperimentData","category":"page"},{"location":"types/#BOSS.ExperimentData","page":"Data Types & Structures","title":"BOSS.ExperimentData","text":"Stores all the data collected during the optimization as well as the parameters and hyperparameters of the model.\n\nSee also: ExperimentDataPrior, ExperimentDataPost\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ExperimentDataPriors structure is used to pass the initial dataset to the BossProblem.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"ExperimentDataPrior","category":"page"},{"location":"types/#BOSS.ExperimentDataPrior","page":"Data Types & Structures","title":"BOSS.ExperimentDataPrior","text":"Stores the initial data.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\n\nSee also: ExperimentDataPost\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ExperimentDataPost types contain the estimated model (hyper)parameters in addition to the dataset. The ExperimentDataMAP structure contains the MAP estimate of the parameters in case a MAP model fitter is used, and the ExperimentDataBI structure contains samples of the parameters in case a Bayesian inference model fitter is used.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"ExperimentDataPost\nExperimentDataMAP\nExperimentDataBI","category":"page"},{"location":"types/#BOSS.ExperimentDataPost","page":"Data Types & Structures","title":"BOSS.ExperimentDataPost","text":"Stores the fitted/samples model parameters in addition to the data matrices X,Y.\n\nSee also: ExperimentDataPrior, ExperimentDataMAP, ExperimentDataBI\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.ExperimentDataMAP","page":"Data Types & Structures","title":"BOSS.ExperimentDataMAP","text":"Stores the data matrices X,Y as well as the optimized model parameters and hyperparameters.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\nθ::Union{Nothing, <:AbstractVector{<:Real}}: Contains the MAP parameters       of the parametric model (or nothing if the model is nonparametric).\nlength_scales::Union{Nothing, <:AbstractMatrix{<:Real}}: Contains the MAP length scales       of the GP as a x_dim×y_dim matrix (or nothing if the model is parametric).\namplitudes::Union{Nothing, <:AbstractVector{<:Real}}: Amplitudes of the GP.\nnoise_std::AbstractVector{<:Real}: The MAP noise standard deviations of each y dimension.\nconsistent::Bool: True iff the parameters (θ, length_scales, amplitudes, noise_std)       have been fitted using the current dataset (X, Y).       Is set to consistent = false after updating the dataset,       and to consistent = true after re-fitting the parameters.\n\nSee also: ExperimentDataBI\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.ExperimentDataBI","page":"Data Types & Structures","title":"BOSS.ExperimentDataBI","text":"Stores the data matrices X,Y as well as the sampled model parameters and hyperparameters.\n\nFields\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\nθ::Union{Nothing, <:AbstractVector{<:AbstractVector{<:Real}}}: Samples of parameters of the parametric model       stored column-wise in a matrix (or nothing if the model is nonparametric).\nlength_scales::Union{Nothing, <:AbstractVector{<:AbstractMatrix{<:Real}}}: Samples   of the length scales of the GP as a vector of x_dim×y_dim matrices   (or nothing if the model is parametric).\namplitudes::Union{Nothing, <:AbstractVector{<:AbstractVector{<:Real}}}: Samples of the amplitudes of the GP.\nnoise_std::AbstractVector{<:AbstractVector{<:Real}}: Samples of the noise standard deviations of each y dimension       stored column-wise in a matrix.\nconsistent::Bool: True iff the parameters (θ, length_scales, amplitudes, noise_std)       have been fitted using the current dataset (X, Y).       Is set to consistent = false after updating the dataset,       and to consistent = true after re-fitting the parameters.\n\nSee also: ExperimentDataMAP\n\n\n\n\n\n","category":"type"},{"location":"types/#Model-Fitter","page":"Data Types & Structures","title":"Model Fitter","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ModelFitter type defines the algorithm used to estimate the model (hyper)parameters.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"ModelFitter\nModelFit","category":"page"},{"location":"types/#BOSS.ModelFitter","page":"Data Types & Structures","title":"BOSS.ModelFitter","text":"Specifies the library/algorithm used for model parameter estimation. Inherit this type to define a custom model-fitting algorithms.\n\nExample: struct CustomFitter <: ModelFitter{MAP} ... end or struct CustomFitter <: ModelFitter{BI} ... end\n\nStructures derived from this type have to implement the following method: estimate_parameters(model_fitter::CustomFitter, problem::BossProblem; info::Bool).\n\nThis method should return a tuple (params, val). The returned params should be a named tuple (θ = ..., length_scale = ..., amplitudes = ..., noise_std = ...) containing the best parameter values (if CustomAlg <: ModelFitter{MAP}) or parameter samples (if CustomAlg <: ModelFitter{BI}). The returned val should be the log likelihood of the parameters (if CustomAlg <: ModelFitter{MAP}), or a vector of log likelihoods of the individual parameter samples (if CustomAlg <: ModelFitter{BI}), or nothing.\n\nSee also: OptimizationMAP, TuringBI\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.ModelFit","page":"Data Types & Structures","title":"BOSS.ModelFit","text":"An abstract type used to differentiate between MAP (Maximum A Posteriori) optimizers and BI (Bayesian Inference) samplers.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The OptimizationMAP model fitter can be used to utilize any optimization algorithm from the Optimization.jl package in order to find the MAP estimate of the (hyper)parameters. (See the example usage.)","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"OptimizationMAP","category":"page"},{"location":"types/#BOSS.OptimizationMAP","page":"Data Types & Structures","title":"BOSS.OptimizationMAP","text":"OptimizationMAP(; kwargs...)\n\nFinds the MAP estimate of the model parameters and hyperparameters using the Optimization.jl package.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Union{Int, Matrix{Float64}}: The number of optimization restarts,       or a vector of tuples (θ, λ, α) containing initial (hyper)parameter values for the optimization runs.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nsoftplus_hyperparams::Bool: If softplus_hyperparams=true then the softplus function       is applied to GP hyperparameters (length-scales & amplitudes) and noise deviations       to ensure positive values during optimization.\nsoftplus_params::Union{Bool, Vector{Bool}}: Defines to which parameters of the parametric       model should the softplus function be applied to ensure positive values.       Supplying a boolean instead of a binary vector turns the softplus on/off for all parameters.       Defaults to false meaning the softplus is applied to no parameters.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The TuringBI model fitter can be used to utilize the Turing.jl library in order to sample the (hyper)parameters from the posterior given by the current dataset.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"TuringBI","category":"page"},{"location":"types/#BOSS.TuringBI","page":"Data Types & Structures","title":"BOSS.TuringBI","text":"TuringBI(; kwargs...)\n\nSamples the model parameters and hyperparameters using the Turing.jl package.\n\nKeywords\n\nsampler::Any: The sampling algorithm used to draw the samples.\nn_adapts::Int: The amount of initial unused 'warmup' samples in each chain.\nsamples_in_chain::Int: The amount of samples used from each chain.\nchain_count::Int: The amount of independent chains sampled.\nleap_size: Every leap_size-th sample is used from each chain. (To avoid correlated samples.)\nparallel: If parallel=true then the chains are sampled in parallel.\n\nSampling Process\n\nIn each sampled chain;\n\nThe first n_adapts samples are discarded.\nFrom the following leap_size * samples_in_chain samples each leap_size-th is kept.\n\nThen the samples from all chains are concatenated and returned.\n\nTotal drawn samples:    'chaincount * (warmup + leapsize * samplesinchain)' Total returned samples: 'chaincount * samplesin_chain'\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SamplingMAP model fitter preforms MAP estimation by sampling the parameters from their priors and maximizing the posterior probability over the samples. This is a trivial model fitter suitable for simple experimentation with BOSS.jl and/or Bayesian optimization. A more sophisticated model fitter such as OptimizationMAP or TuringBI should be used to solve real problems.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SamplingMAP","category":"page"},{"location":"types/#BOSS.SamplingMAP","page":"Data Types & Structures","title":"BOSS.SamplingMAP","text":"SamplingMAP()\n\nOptimizes the model parameters by sampling them from their prior distributions and selecting the best sample in sense of MAP.\n\nKeywords\n\nsamples::Int: The number of drawn samples.\nparallel::Bool: The sampling is performed in parallel if parallel=true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The RandomMAP model fitter samples random parameter values from their priors. It does NOT optimize for the most probable parameters in any way. This model fitter is provided solely for easy experimentation with BOSS.jl and should not be used to solve problems.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"RandomMAP","category":"page"},{"location":"types/#BOSS.RandomMAP","page":"Data Types & Structures","title":"BOSS.RandomMAP","text":"RandomMAP()\n\nReturns random model parameters sampled from their respective priors.\n\nCan be useful with RandomSelectAM to avoid unnecessary model parameter estimations.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SampleOptMAP model fitter combines the SamplingMAP and OptimizationMAP. It first samples many model parameter samples from their priors, and subsequently runs multiple optimization runs initiated at the best samples.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SampleOptMAP","category":"page"},{"location":"types/#BOSS.SampleOptMAP","page":"Data Types & Structures","title":"BOSS.SampleOptMAP","text":"SampleOptMAP(; kwargs...)\nSampleOptMAP(::SamplingMAP, ::OptimizationMAP)\n\nCombines SamplingMAP and OptimizationMAP to first sample many parameter samples from the prior, and subsequently start multiple optimization runs initialized from the best samples.\n\nKeywords\n\nsamples::Int: The number of drawn samples.\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true, then both the sampling and the optimization are performed in parallel.\nsoftplus_hyperparams::Bool: If softplus_hyperparams=true then the softplus function       is applied to GP hyperparameters (length-scales & amplitudes) and noise deviations       to ensure positive values during optimization.\nsoftplus_params::Union{Bool, Vector{Bool}}: Defines to which parameters of the parametric       model should the softplus function be applied to ensure positive values.       Supplying a boolean instead of a binary vector turns the softplus on/off for all parameters.       Defaults to false meaning the softplus is applied to no parameters.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Maximizer","page":"Data Types & Structures","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The AcquisitionMaximizer type is used to define the algorithm used to maximize the acquisition function.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"AcquisitionMaximizer","category":"page"},{"location":"types/#BOSS.AcquisitionMaximizer","page":"Data Types & Structures","title":"BOSS.AcquisitionMaximizer","text":"Specifies the library/algorithm used for acquisition function optimization. Inherit this type to define a custom acquisition maximizer.\n\nExample: struct CustomAlg <: AcquisitionMaximizer ... end\n\nStructures derived from this type have to implement the following method: maximize_acquisition(acq_maximizer::CustomAlg, acq::AcquisitionFunction, problem::BossProblem, options::BossOptions).\n\nThis method should return a tuple (x, val). The returned x is the point of the input domain which maximizes the given acquisition function acq (as a vector), or a batch of points (as a column-wise matrix). The returned val is the acquisition value acq(x), or the values acq.(eachcol(x)) for each point of the batch, or nothing (depending on the acquisition maximizer implementation).\n\nSee also: OptimizationAM\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The OptimizationAM can be used to utilize any optimization algorithm from the Optimization.jl package.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"OptimizationAM","category":"page"},{"location":"types/#BOSS.OptimizationAM","page":"Data Types & Structures","title":"BOSS.OptimizationAM","text":"OptimizationAM(; kwargs...)\n\nMaximizes the acquisition function using the Optimization.jl library.\n\nCan handle constraints on x if according optimization algorithm is selected.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Union{<:Int, <:AbstractMatrix{<:Real}}: The number of optimization restarts,       or a matrix of optimization intial points as columns.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nautodiff:SciMLBase.AbstractADType:: The automatic differentiation module       passed to Optimization.OptimizationFunction.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The GridAM maximizes the acquisition function by evaluating all points on a fixed grid of points. This is a trivial acquisition maximizer suitable for simple experimentation with BOSS.jl and/or Bayesian optimization. More sophisticated acquisition maximizers such as OptimizationAM should be used to solve real problems.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"GridAM","category":"page"},{"location":"types/#BOSS.GridAM","page":"Data Types & Structures","title":"BOSS.GridAM","text":"GridAM(problem, steps; kwargs...)\n\nMaximizes the acquisition function by checking a fine grid of points from the domain.\n\nExtremely simple optimizer which can be used for simple problems or for debugging. Not suitable for problems with high dimensional domain.\n\nCan be used with constraints on x.\n\nArguments\n\nproblem::BossProblem: Provide your defined optimization problem.\nsteps::Vector{Float64}: Defines the size of the grid gaps in each x dimension.\n\nKeywords\n\nparallel::Bool: If parallel=true then the optimization is parallelized. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SamplingAM samples random candidate points from the given x_prior distribution and selects the sample with maximal acquisition value.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SamplingAM","category":"page"},{"location":"types/#BOSS.SamplingAM","page":"Data Types & Structures","title":"BOSS.SamplingAM","text":"SamplingAM(; kwargs...)\n\nOptimizes the acquisition function by sampling candidates from the user-provided prior, and returning the sample with the highest acquisition value.\n\nKeywords\n\nx_prior::MultivariateDistribution: The prior over the input domain used to sample candidates.\nsamples::Int: The number of samples to be drawn and evaluated.\nparallel::Bool: If parallel=true then the sampling is parallelized. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The RandomAM simply returns a random point. It does NOT perform any optimization. This acquisition maximizer is provided solely for easy experimentation with BOSS.jl and should not be used to solve problems.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"RandomAM","category":"page"},{"location":"types/#BOSS.RandomAM","page":"Data Types & Structures","title":"BOSS.RandomAM","text":"RandomAM()\n\nSelects a random interior point instead of maximizing the acquisition function. Can be used for method comparison.\n\nCan handle constraints on x, but does so by generating random points in the box domain until a point satisfying the constraints is found. Therefore it can take a long time or even get stuck if the constraints are very tight.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SampleOptAM samples many candidate points from the given x_prior distribution, and subsequently performs multiple optimization runs initiated from the best samples.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SampleOptAM","category":"page"},{"location":"types/#BOSS.SampleOptAM","page":"Data Types & Structures","title":"BOSS.SampleOptAM","text":"SampleOptAM(; kwargs...)\n\nOptimizes the acquisition function by first sampling candidates from the user-provided prior, and then running multiple optimization runs initiated from the samples with the highest acquisition values.\n\nKeywords\n\nx_prior::MultivariateDistribution: The prior over the input domain used to sample candidates.\nsamples::Int: The number of samples to be drawn and evaluated.\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true, both the sampling and individual optimization runs       are performed in parallel.\nautodiff:SciMLBase.AbstractADType:: The automatic differentiation module       passed to Optimization.OptimizationFunction.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SequentialBatchAM can be used as a wrapper of any of the other acquisition maximizers. It returns a batch of promising points for future evaluations instead of a single point, and thus allows for evaluation of the objective function in batches.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"SequentialBatchAM","category":"page"},{"location":"types/#BOSS.SequentialBatchAM","page":"Data Types & Structures","title":"BOSS.SequentialBatchAM","text":"SequentialBatchAM(::AcquisitionMaximizer, ::Int)\nSequentialBatchAM(; am, batch_size)\n\nProvides multiple candidates for batched objective function evaluation.\n\nSelects the candidates sequentially by iterating the following steps:\n\n) Use the 'inner' acquisition maximizer to select a candidatex`.\nExtend the dataset with a 'speculative' new data point\ncreated by taking the candidate x and the posterior predictive mean of the surrogate ŷ.\nIf batch_size candidates have been selected, return them.\nOtherwise, goto step 1).\n\nKeywords\n\nam::AcquisitionMaximizer: The inner acquisition maximizer.\nbatch_size::Int: The number of candidates to be selected.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Function","page":"Data Types & Structures","title":"Acquisition Function","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The acquisition function is defined using the AcquisitionFunction type.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"AcquisitionFunction","category":"page"},{"location":"types/#BOSS.AcquisitionFunction","page":"Data Types & Structures","title":"BOSS.AcquisitionFunction","text":"Specifies the acquisition function describing the \"quality\" of a potential next evaluation point. Inherit this type to define a custom acquisition function.\n\nExample: struct CustomAcq <: AcquisitionFunction ... end\n\nStructures derived from this type have to implement the following method: (acquisition::CustomAcq)(problem::BossProblem, options::BossOptions)\n\nThis method should return a function acq(x::AbstractVector{<:Real}) = val::Real, which is maximized to select the next evaluation function of blackbox function in each iteration.\n\nSee also: ExpectedImprovement\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ExpectedImprovement defines the expected improvement acquisition function. See [1] for more information.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"ExpectedImprovement","category":"page"},{"location":"types/#BOSS.ExpectedImprovement","page":"Data Types & Structures","title":"BOSS.ExpectedImprovement","text":"ExpectedImprovement(; kwargs...)\n\nThe expected improvement (EI) acquisition function.\n\nFitness function must be defined as a part of the problem definition in order to use EI. (See Fitness.)\n\nMeasures the quality of a potential evaluation point x as the expected improvement in best-so-far achieved fitness by evaluating the objective function at y = f(x).\n\nIn case of constrained problems, the expected improvement is additionally weighted by the probability of feasibility of y. I.e. the probability that all(cons(y) .> 0.).\n\nIf the problem is constrained on y and no feasible point has been observed yet, then the probability of feasibility alone is returned as the acquisition function.\n\nRather than using the actual evaluations (xᵢ,yᵢ) from the dataset, the best-so-far achieved fitness is calculated as the maximum fitness among the means ̂yᵢ of the posterior predictive distribution of the model evaluated at xᵢ. This is a simple way to handle evaluation noise which may not be suitable for problems with substantial noise. In case of Bayesian Inference, an averaged posterior of the model posterior samples is used for the prediction of ŷᵢ.\n\nKeywords\n\nϵ_samples::Int: Controls how many samples are used to approximate EI.       The ϵ_samples keyword is ignored unless MAP model fitter and NonlinFitness are used!       In case of BI model fitter, the number of samples is instead set equal to the number of posterior samples.       In case of LinearFitness, the expected improvement can be calculated analytically.\ncons_safe::Bool: If set to true, the acquisition function acq(x) is made 'constraint-safe'       by checking the bounds and constraints during each evaluation.       Set cons_safe to true if the evaluation of the model at exterior points       may cause errors or nonsensical values.       You may set cons_safe to false if the evaluation of the model at exterior points       can provide useful information to the acquisition maximizer and does not cause errors.       Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/#Termination-Conditions","page":"Data Types & Structures","title":"Termination Conditions","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The TermCond type is used to define the termination condition of the BO procedure.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"TermCond","category":"page"},{"location":"types/#BOSS.TermCond","page":"Data Types & Structures","title":"BOSS.TermCond","text":"Specifies the termination condition of the whole BOSS algorithm. Inherit this type to define a custom termination condition.\n\nExample: struct CustomCond <: TermCond ... end\n\nStructures derived from this type have to implement the following method: (cond::CustomCond)(problem::BossProblem)\n\nThis method should return true to keep the optimization running and return false once the optimization is to be terminated.\n\nSee also: IterLimit\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The IterLimit terminates the procedure after a predefined number of iterations.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"IterLimit","category":"page"},{"location":"types/#BOSS.IterLimit","page":"Data Types & Structures","title":"BOSS.IterLimit","text":"IterLimit(iter_max::Int)\n\nTerminates the BOSS algorithm after predefined number of iterations.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"types/#Miscellaneous","page":"Data Types & Structures","title":"Miscellaneous","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossOptions structure is used to define miscellaneous hyperparameters of the BOSS.jl package.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"BossOptions","category":"page"},{"location":"types/#BOSS.BossOptions","page":"Data Types & Structures","title":"BOSS.BossOptions","text":"BossOptions(; kwargs...)\n\nStores miscellaneous settings of the BOSS algorithm.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the BOSS algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::BossCallback: If provided, callback(::BossProblem; kwargs...)       will be called before the BO procedure starts and after every iteration.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossCallback type is used to pass callbacks which will be called in every iteration of the BO procedure (and once before the procedure starts).","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"BossCallback\nNoCallback","category":"page"},{"location":"types/#BOSS.BossCallback","page":"Data Types & Structures","title":"BOSS.BossCallback","text":"If an object cb of type BossCallback is passed to BossOptions, the method shown below will be called before the BO procedure starts and after every iteration.\n\ncb(problem::BossProblem;\n    model_fitter::ModelFitter,\n    acq_maximizer::AcquisitionMaximizer,\n    acquisition::AcquisitionFunction,\n    term_cond::TermCond,\n    options::BossOptions,\n    first::Bool,\n)\n\nThe kwargs first is true on the first callback before the BO procedure starts, and is false on all subsequent callbacks after each iteration.\n\nSee PlotCallback for an example usage of this feature for plotting.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NoCallback","page":"Data Types & Structures","title":"BOSS.NoCallback","text":"NoCallback()\n\nDoes nothing.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The PlotCallback provides plots the state of the BO procedure in every iteration. It currently only supports one-dimensional input spaces.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"PlotCallback","category":"page"},{"location":"types/#BOSS.PlotCallback","page":"Data Types & Structures","title":"BOSS.PlotCallback","text":"PlotOptions(Plots; kwargs...)\n\nIf PlotOptions is passed to BossOptions as callback, the state of the optimization problem is plotted in each iteration. Only works with one-dimensional x domains but supports multi-dimensional y.\n\nArguments\n\nPlots::Module: Evaluate using Plots and pass the Plots module to PlotsOptions.\n\nKeywords\n\nf_true::Union{Nothing, Function}: The true objective function to be plotted.\npoints::Int: The number of points in each plotted function.\nxaxis::Symbol: Used to change the x axis scale (:identity, :log).\nyaxis::Symbol: Used to change the y axis scale (:identity, :log).\ntitle::String: The plot title.\n\n\n\n\n\n","category":"type"},{"location":"types/#References","page":"Data Types & Structures","title":"References","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"[1] Bobak Shahriari et al. “Taking the human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104.1 (2015), pp. 148–175","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This page contains the documentation for all exported functions.","category":"page"},{"location":"functions/#Main-Function","page":"Functions","title":"Main Function","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The main function bo!(::BossProblem; kwargs...) performs the Bayesian optimization. It augments the dataset and updates the model parameters and/or hyperparameters stored in problem.data.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"bo!","category":"page"},{"location":"functions/#BOSS.bo!","page":"Functions","title":"BOSS.bo!","text":"bo!(problem::BossProblem{Function}; kwargs...)\nx = bo!(problem::BossProblem{Missing}; kwargs...)\n\nRun the Bayesian optimization procedure to solve the given optimization problem or give a recommendation for the next evaluation point if problem.f == missing.\n\nArguments\n\nproblem::BossProblem: Defines the optimization problem.\n\nKeywords\n\nmodel_fitter::ModelFitter: Defines the algorithm used to estimate model parameters.\nacq_maximizer::AcquisitionMaximizer: Defines the algorithm used to maximize the acquisition function.\nacquisition::AcquisitionFunction: Defines the acquisition function maximized to select       promising candidates for further evaluation.\nterm_cond::TermCond: Defines the termination condition.\noptions::BossOptions: Defines miscellaneous options and hyperparameters.\n\nReferences\n\nBossProblem, ModelFitter, AcquisitionMaximizer, TermCond, BossOptions\n\nExamples\n\nSee 'https://github.com/Sheld5/BOSS.jl/tree/master/scripts' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The following diagram showcases the pipeline of the main function. The package can be used in two modes;","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The \"BO mode\" is used if the objective function is defined within the BossProblem. In this mode, BOSS performs the standard Bayesian optimization procedure while querying the objective function for new points.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The \"Recommender mode\" is used if the objective function is missing. In this mode, BOSS performs a single iteration of the Bayesian optimization procedure and returns a recommendation for the next evaluation point. The user can evaluate the objective function manually, use the method augment_dataset! to add the result to the data, and call BOSS again for a new recommendation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"    \n  (Image: BOSS Pipeline)  \n    ","category":"page"},{"location":"functions/#Utility-Functions","page":"Functions","title":"Utility Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"augment_dataset!\nmodel_posterior\nmodel_posterior_slice\naverage_posteriors\nresult","category":"page"},{"location":"functions/#BOSS.augment_dataset!","page":"Functions","title":"BOSS.augment_dataset!","text":"augment_dataset!(data::ExperimentDataPost, x::AbstractVector{<:Real}, y::AbstractVector{<:Real})\naugment_dataset!(data::ExperimentDataPost, X::AbstractMatrix{<:Real}, Y::AbstractMatrix{<:Real})\n\nAdd one (as vectors) or more (as matrices) datapoints to the dataset.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.model_posterior","page":"Functions","title":"BOSS.model_posterior","text":"model_posterior(::BossProblem) -> (x -> mean, std)\n\nReturn the posterior predictive distribution of the Gaussian Process.\n\nThe posterior is a function predict(x) -> (mean, std) which gives the mean and std of the predictive distribution as a function of x.\n\nSee also: model_posterior_slice\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.model_posterior_slice","page":"Functions","title":"BOSS.model_posterior_slice","text":"model_posterior_slice(::BossProblem, slice::Int) -> (x -> mean, std)\n\nReturn the posterior predictive distributions of the given slice output dimension.\n\nIn case of a Gaussian process model (or a semiparametric model), using model_posterior_slice is more efficient than model_posterior if one is only interested in the predictive distribution of a certain output dimension.\n\nSee also: model_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.average_posteriors","page":"Functions","title":"BOSS.average_posteriors","text":"Return an averaged posterior predictive distribution of the given posteriors.\n\nThe posterior is a function predict(x) -> (mean, std) which gives the mean and standard deviation of the predictive distribution as a function of x.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.result","page":"Functions","title":"BOSS.result","text":"result(problem) -> (x, y)\n\nReturn the best found point (x, y).\n\nReturns the point (x, y) from the dataset of the given problem such that y satisfies the constraints and fitness(y) is maximized. Returns nothing if the dataset is empty or if no feasible point is present.\n\nDoes not check whether x belongs to the domain as no exterior points should be present in the dataset.\n\n\n\n\n\n","category":"function"},{"location":"#BOSS","page":"BOSS","title":"BOSS","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"BOSS stands for \"Bayesian Optimization with Semiparametric Surrogate\". BOSS.jl is a Julia package for Bayesian optimization. It provides a compact way to define an optimization problem and a surrogate model, and solve the problem. It allows changing the algorithms used for the subtasks of fitting the surrogate model and optimizing the acquisition function. Simple interfaces are defined for the use of custom surrogate models and/or algorithms for the subtasks.","category":"page"},{"location":"#Bayesian-Optimization","page":"BOSS","title":"Bayesian Optimization","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"Bayesian optimization is an abstract algorithm for blackbox optimization (or active learning) with expensive-to-evaluate objective functions. The general procedure is showcased by the pseudocode below. [1]","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"    \n  (Image: Bayesian optimization)  \n    ","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"The real-valued acquisition function alpha(x) defines the utility of a potential point for the next evaluation. Different acquisition functions are suitable for optimization and active learning problems. We maximize the acquisition function to select the most useful point for the next evaluation.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"Once the optimum x_n+1 of the acquisition function is obtained, we evaluate the blackbox objective function y_n+1 gets f(x_n+1) and we augment the dataset.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"Finally, we update the surrogate model according to the augmented dataset.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"See [1] for more information on Bayesian optimization.","category":"page"},{"location":"#Optimization-Problem","page":"BOSS","title":"Optimization Problem","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"The problem is defined using the BossProblem structure and it follows the formalization below.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"We have some (noisy) blackbox objective function","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"y = f(x) = f_t(x) + epsilon ","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"where epsilon sim mathcalN(0 sigma_f^2) is a Gaussian noise. We are able to evaluate f(x_i) and obtain a noisy realization","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"y_i sim mathcalN(f_t(x_i) sigma_f^2) ","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"Our goal is to solve the following optimization problem","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"beginaligned\ntextmax   textfit(y) \ntextst   y  y_textmax \n x in textDomain \nendaligned","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"where textfit(y) is a real-valued fitness function defined on the outputs, y_textmax is a vector defining constraints on outputs, and textDomain defines constraints on inputs.","category":"page"},{"location":"#Active-Learning-Problem","page":"BOSS","title":"Active Learning Problem","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"The BOSS.jl package currently only supports optimization problems out-of-the-box. However, BOSS.jl can be adapted for active learning easily by defining a suitable acquisition function (such as information gain or Kullback-Leibler divergence) to use instead of the expected improvement (see AcquisitionFunction). An acquisition function for active learning will usually not require the fitness function to be defined, so the fitness function can be omitted during problem definition (see BossProblem).","category":"page"},{"location":"#Surrogate-Model","page":"BOSS","title":"Surrogate Model","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"The surrogate model approximates the objective function based on the available data. It is defined using the SurrogateModel type. The BOSS.jl package provides a Parametric model, a Nonparametric model, and a Semiparametric model combining the previous two.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"The predictive distribution of the Parametric model","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"y sim mathcalN(m(x hattheta) hatsigma_f^2)","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"is given by the parametric function m(x theta), the parameter vector hattheta, and the estimated evaluation noise deviations hatsigma_f. The model is defined by the parametric function m(x theta) together with parameter priors theta_i sim p(theta_i). The parameters hattheta and the noise deviations hatsigma_f are estimated based on the current dataset.","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"The Nonparametric model is just an alias for the GaussianProcess model. Gaussian process (GP) is a nonparametric model, so its predictive distribution is based on the whole dataset instead of some vector of parameters. The predictive distribution is given by equations 29, 30 in [1]. The model is defined by defining priors over all its hyperparameters (length scales, amplitudes).","category":"page"},{"location":"","page":"BOSS","title":"BOSS","text":"The Semiparametric model combines the previous two models. It is a Gaussian process, but uses the parametric model as the prior mean of the GP (the mu_0(x) function in [1]). An alternative way of interpreting the semiparametric model is that it fits the data using a parametric model and uses a Gaussian process to model the residual errors of the parametric model. The model is defined by defining both the Parametric and Nonparametric models.","category":"page"},{"location":"#References","page":"BOSS","title":"References","text":"","category":"section"},{"location":"","page":"BOSS","title":"BOSS","text":"[1] Bobak Shahriari et al. “Taking the human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104.1 (2015), pp. 148–175","category":"page"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"An illustrative problem is solved using BOSS.jl here. The source code is available at github.","category":"page"},{"location":"example/#The-Problem","page":"Example","title":"The Problem","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We have an expensive-to-evaluate blackbox function blackbox(x) -> (y, z).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function blackbox(x; noise_std=0.1)\n    y = exp(x[1]/10) * cos(2*x[1])\n    z = (1/2)^6 * (x[1]^2 - (15.)^2)\n    \n    y += rand(Normal(0., noise_std))\n    z += rand(Normal(0., noise_std))\n\n    return [y,z]\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We wish to maximize y such that x ∈ <0, 20> (constraint on input) and z < 0 (constraint on output).","category":"page"},{"location":"example/#Problem-Definition","page":"Example","title":"Problem Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"First, we define the problem as an instance of BossProblem.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"problem() = BossProblem(;\n    fitness = LinFitness([1, 0]),           # maximize y\n    f = blackbox,\n    domain = Domain(;\n        bounds = ([0.], [20.]),             # x ∈ <0, 20>\n    ),\n    y_max = [Inf, 0.],                      # z < 0\n    model = nonparametric(), # or `parametric()` or `semiparametric()`\n    noise_std_priors = noise_std_priors(),\n    data = init_data(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use Fitness to define the objective. Here, the LinFitness([1, 0]) specifies that we wish to maximize 1*y + 0*z. (See also NonlinFitness.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword f to provide the blackbox objective function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the Domain structure to define the constraints on inputs.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword y_max to define the constraints on outputs.","category":"page"},{"location":"example/#Surrogate-Model-Definition","page":"Example","title":"Surrogate Model Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Now we define the surrogate model used to approximate the objective function based on the available data from previous evaluations.","category":"page"},{"location":"example/#Gaussian-Process","page":"Example","title":"Gaussian Process","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Usually, we will use a Gaussian process.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"nonparametric() = GaussianProcess(;\n    kernel = BOSS.Matern32Kernel(),\n    amp_priors = amplitude_priors(),\n    length_scale_priors = length_scale_priors(),\n)","category":"page"},{"location":"example/#Parametric-Model","page":"Example","title":"Parametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"If we have some knowledge about the blackbox function, we can define a parametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"parametric() = NonlinModel(;\n    predict = (x, θ) -> [\n        θ[1] * x[1] * cos(θ[2] * x[1]) + θ[3],\n        0.,\n    ],\n    param_priors = fill(Normal(0., 1.), 3),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The function predict(x, θ) -> y defines our parametric model where θ are the model parameters which will be fitted based on the data.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The keyword param_priors is used to define priors on the model parameters θ. The priors can be used to include our expert knowledge, to regularize the model, or a uniform prior can be used to not bias the model fit.","category":"page"},{"location":"example/#Semiparametric-Model","page":"Example","title":"Semiparametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can use the parametric model together with a Gaussian process to define the semiparametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"semiparametric() = Semiparametric(\n    parametric(),\n    nonparametric(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"This allows us to leverage our expert knowledge incorporated in the parametric model while benefiting from the flexibility of the Gaussian process.","category":"page"},{"location":"example/#Hyperparameter-Priors","page":"Example","title":"Hyperparameter Priors","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We need to define all hyperparameters. Instead of defining scalar values, we will define priors over them and let BOSS fit their values based on the data. This alleviates the importance of our choice and allows for Bayesian inference if we wish to use it.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(If one wants to define some hyperparameters as scalars instead, a Dirac prior can be used and the hyperparameters will be skipped from model fitting.)","category":"page"},{"location":"example/#Evaluation-Noise","page":"Example","title":"Evaluation Noise","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"BOSS assumes Gaussian evaluation noise on the objective blackbox function. Noise std priors define our belief about the standard deviation of the noise of each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"noise_std_priors() = fill(truncated(Normal(0., 0.1); lower=0.), 2)\n# noise_std_priors() = fill(Dirac(0.1), 2)","category":"page"},{"location":"example/#Amplitude","page":"Example","title":"Amplitude","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The amplitude of the Gaussian process expresses the expected deviation of the output values. We again define an amplitude prior for each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"amplitude_priors() = fill(truncated(Normal(0., 5.); lower=0.), 2)\n# amplitude_priors() = fill(Dirac(5.), 2)","category":"page"},{"location":"example/#Length-Scales","page":"Example","title":"Length Scales","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Informally, the length scales of the Gaussian process define how far within the input domain does the model extrapolate the information obtained from the dataset. For each output dimension, we define a multivariate prior over all input dimensions. (In our case two 1-variate priors.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"length_scale_priors() = fill(Product([truncated(Normal(0., 20/3); lower=0.)]), 2)\n# length_scale_priors() = fill(Product(fill(Dirac(1.), 1)), 2)","category":"page"},{"location":"example/#Model-Fitter","page":"Example","title":"Model Fitter","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to fit the model hyperparameters using the ModelFitter type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can fit the hyperparameters in a MAP fashion using the OptimizationMAP model fitter together with any algorithm from Optimization.jl and its extensions.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using OptimizationPRIMA\n\nmap_fitter() = OptimizationMAP(;\n    algorithm = NEWUOA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Or we can use Bayesian inference and sample the parameters from their posterior (given by the priors and the data likelihood) using the TuringBI model fitter.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bi_fitter() = TuringBI(;\n    sampler = BOSS.PG(20),\n    warmup = 100,\n    samples_in_chain = 10,\n    chain_count = 8,\n    leap_size = 5,\n    parallel = true,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also SamplingMAP and RandomMAP for more trivial model fitters suitable for simple experimentation with the package.","category":"page"},{"location":"example/#Acquisition-Maximizer","page":"Example","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to maximize the acquisition function (in order to select the next evaluation point) by using the AcquisitionMaximizer type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can use the OptimizationAM maximizer together with any algorithm from Optimization.jl.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"acq_maximizer() = OptimizationAM(;\n    algorithm = BOBYQA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Make sure to use an algorithm suitable for the given domain. (For example, in our case the domain is bounded by box constraints only, so we the BOBYQA optimization algorithm designed for box constraints problems.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also GridAM, RandomAM for more trivial acquisition maximizers suitable for simple experimentation with the package.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The SequentialBatchAM can be used to wrap any of the other acquisition maximizers to enable objective function evaluation in batches.","category":"page"},{"location":"example/#Acquisition-Function","page":"Example","title":"Acquisition Function","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The acquisition function defines how the next evaluation point is selected in each iteration. The acquisition function is maximized by the acquisition maximizer algorithm (discussed in the previous section).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Currently, the only implemented acquisition function is the ExpectedImprovement acquisition most commonly used in Bayesian optimization.","category":"page"},{"location":"example/#Miscellaneous","page":"Example","title":"Miscellaneous","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we can define the termination condition using the TermCond type. Currently, the only available termination condition is the trivial IterLimit condition. (However, one can simply define his own termination condition by extending the TermCond type.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The BossOptions structure can be used to change other miscellaneous settings.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The PlotCallback can be provided in BossOptions to enable plotting of the BO procedure. This can be useful for initial experimentation with the package. Note that the plotting only works for 1-dimensional input domains.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Plots\n\noptions() = BossOptions(;\n    info = true,\n    callback = PlotCallback(Plots;\n        f_true = x->blackbox(x; noise_std=0.),\n    ),\n)","category":"page"},{"location":"example/#Run-BOSS","page":"Example","title":"Run BOSS","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Once we define the problem and all hyperparameters, we can run the BO procedure by calling the bo! function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bo!(problem();\n    model_fitter = map_fitter(), # or `bi_fitter()`\n    acq_maximizer = acq_maximizer(),\n    acquisition = ExpectedImprovement(),\n    term_cond = IterLimit(10),\n    options = options(),\n)","category":"page"}]
}
